{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand\\given[1][]{\\:#1\\mid\\:}\n",
    "\\def\\*#1{\\mathbf{#1}}\n",
    "\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}}\n",
    "\\newcommand{\\argmax}{\\mathop{\\mathrm{argmax}}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we observe data $\\*x = (x_1, ..., x_m)$ of $m$ \"features\" and we want to assign probabilities to each of $K$ different classes $\\{C_1, ..., C_K\\}$.\n",
    "\n",
    "Bayes' theorem allows us to invert this as:\n",
    "\n",
    "$$\n",
    "p(C_k \\given \\*x) = \\frac{p(C_k) p(\\*x \\given C_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "Recall that we define these plain English terms:\n",
    "\n",
    "$$\n",
    "\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{evidence}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayes classification\n",
    "\n",
    "We can construct a classifier from this conditional probability with a decision rule. One common rule is to choose the hypothesis (class) that is most probable. This is known as the \"maximum a posteriori\" or MAP decision rule. Applying this gives us a **Bayes classifier**: a function that assigns the class label $\\hat{y} = C_k$, where $k$ is given by:\n",
    "\n",
    "$$\n",
    "\\argmax_{k \\in \\{1, ..., K\\}} p(C_k) p(\\*x \\given C_k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "What about the evidence term in the denominator? \n",
    "This is the irrelevant to the classification problem; it is the same for all classes $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Uncertainty estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that the MAP decision rule **throws away information**. The most probable class is only one outcome of the Bayesian formulation of the classification problem above. We can also also read off the probabilities of other classes, the odds (or log odds) of the data belonging to one class or another, etc. We will come back to this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelling the class likelihoods\n",
    "\n",
    "How do we go about modelling the class likelihoods $p(\\* x \\given C_k)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In relatively low-dimensional problems, we can infer these using **density estimation** (for example, a histogram-smoothing procedure like kernel density estimation).\n",
    "\n",
    "If we have many features (if $m$ is large), our models for the likelihoods $p(\\*x \\given C_k)$ will be high-dimensional. Even with \"big data\" $n \\approx 10^{12}$ or $n \\approx 10^{15}$, the number of data points in each hypercube of feature space will decrease exponentially with $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise: a full Bayesian classifier based on scikit-learn\n",
    "\n",
    "Here we will sketch an implementation of a full Bayesian model for classification. This would suit low-dimensional continuous or discrete problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn has a few algorithms for density estimation:\n",
    "- kernel density\n",
    "- Gaussian mixtures\n",
    "- etc.\n",
    "\n",
    "This can provide one part of the full Bayesian model: the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kde = KernelDensity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kde.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kde.__class__.__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise: look at the help for sklearn's `BaseEstimator` interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "Classifiers in scikit-learn have a different interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression   # a classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def non_special(method_name):\n",
    "    return not(method_name.startswith('__') and method_name.endswith('__'))\n",
    "\n",
    "list(filter(non_special, dir(LogisticRegression)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "LogisticRegression.predict_proba??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_log_proba(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "LogisticRegression.score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "SVC.score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BaseNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "BaseNB??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils import check_array, check_X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u = unique_labels([1, 2, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "find([1, 2, 1, 3, 5], u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extended exercise: implement a `BayesClassifier` class that inherits from `BaseNB`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solution skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BayesClassifier(BaseNB):\n",
    "    \"\"\"\n",
    "    p(C | X) \\prop p(X | C) p(C)\n",
    "    \n",
    "    Performs classification using MAP and\n",
    "    also uncertainty estimation based on the full model.\n",
    "    \n",
    "    Has two components, both themselves scikit-learn Esimator objects:\n",
    "    1. density_estimator: p(X | C). A class. This will be instantiated for each class.\n",
    "       It must have a 'predict_log_proba' method.\n",
    "       \n",
    "    2. priors: ndarray of length len(C).\n",
    "    \"\"\"\n",
    "    def __init__(self, density_estimator, priors):\n",
    "        assert hasattr(density_estimator, 'score_samples')\n",
    "        self.likelihood_fn = density_estimator\n",
    "        self.prior = priors\n",
    "        self.n_classes = len(priors)\n",
    "        self.conditional_models = {}\n",
    "        #self.log_likelihoods = np.zeros(self.n_classes)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a model for each class.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, ensure_min_samples=2, estimator=self)\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        indices = find(y, self.classes_)\n",
    "        for c in range(self.n_classes):\n",
    "            rows = X[indices[c]]\n",
    "            est = self.likelihood_fn()\n",
    "            est.fit(rows)\n",
    "            self.conditional_models[c] = est\n",
    "    \n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Compute the unnormalized posterior log probability of X\n",
    "\n",
    "        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n",
    "        shape [n_classes, n_samples].\n",
    "\n",
    "        Input is passed to _joint_log_likelihood as-is by predict,\n",
    "        predict_proba and predict_log_proba.\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        joint_log_likelihood = []\n",
    "        for i in range(np.size(self.classes_)):\n",
    "            log_p_c = np.log(self.class_prior_[i])\n",
    "            log_p_x_given_c = self.conditional_models[i](X)\n",
    "            joint_log_likelihood.append(log_p_c + log_p_x_given_c)\n",
    "\n",
    "        joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "        return joint_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Try this out with e.g. the iris data or diabetes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = fetch_mldata('diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(diabetes['COL_NAMES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Simplifying assumption for higher dimensions: \"Naive\" Bayes\n",
    "\n",
    "A family of classifiers known as **Naive Bayes** simplifies the likelihood model (at the expense of its expressive power) by making the (strong) simplifying assumption of conditional independence:\n",
    "\n",
    "$$\n",
    "p(\\*x \\given C_k) \\approx \\prod_{i=1}^m p(x_i \\given C_k)\n",
    "$$\n",
    "\n",
    "In addition, further simplifying assumptions are sometimes made for various problems:\n",
    "- that discrete binary values (booleans) are generated from **Bernoulli** trials.\n",
    "- that discrete values representing frequencies of events are distributed according to a **multinomial** distribution (a generalization of the Bernoulli and binomial distributions).\n",
    "- that continuous values associated with each class are **normally** distributed (\"Gaussian\").\n",
    "\n",
    "#### Recall that these assumptions corresponding to certain choices of **prior information** expressed as feature constraints in exponential-family models:\n",
    "\n",
    "- Bernoulli: constraint $E(K) = p$ for sample space $k \\in \\{0,1\\}$.\n",
    "- Multinomial: XXXXXX\n",
    "- Gaussian: constraints on $E(X)$ and $E(X^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplifying assumption for higher dimensions: \"Naive\" Bayes\n",
    "\n",
    "A family of classifiers known as **Naive Bayes** simplifies the likelihood model (at the expense of its expressive power) by making the (strong) simplifying assumption of conditional independence:\n",
    "\n",
    "$$\n",
    "p(\\*x \\given C_k) \\approx \\prod_{i=1}^m p(x_i \\given C_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, further simplifying assumptions are sometimes made for various problems:\n",
    "- that discrete binary values (booleans) are generated from **Bernoulli** trials.\n",
    "- that discrete values representing frequencies of events are distributed according to a **multinomial** distribution (a generalization of the Bernoulli and binomial distributions).\n",
    "- that continuous values associated with each class are **normally** distributed (\"Gaussian\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Recall what these assumptions mean:\n",
    "\n",
    "These assumptions correspond to certain choices of **prior information** expressed as feature constraints in exponential-family models:\n",
    "\n",
    "- Bernoulli: constraint $E(K) = p$ for sample space $k \\in \\{0,1\\}$.\n",
    "- Gaussian: constraints on $E(X)$ and $E(X^2)$\n",
    "- etc.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
